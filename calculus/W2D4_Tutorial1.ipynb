{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "okLKkmlc6NeX"
   },
   "source": [
    "# Tutorial 1: Differentiation and Integration\n",
    "**Week 2, Day 4: Calculus**\n",
    "\n",
    "**This tutorial is modified from Neuromatch Academy**\n",
    "\n",
    "The original code can be found in the following link:(https://colab.research.google.com/github/NeuromatchAcademy/precourse/blob/main/tutorials/W0D4_Calculus/student/W0D4_Tutorial1.ipynb)\n",
    "\n",
    "__Content creators:__ John S Butler, Arvind Kumar with help from Ella Batty\n",
    "\n",
    "__Content reviewers:__  Aderogba Bayo, Tessy Tom, Matt McCann\n",
    "\n",
    "__Production editors:__ Matthew McCann, Ella Batty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "tqapn6HL6Ner"
   },
   "source": [
    "**Our 2023 Sponsor:**\n",
    "\n",
    "<img src='https://www.ias.edu/sites/default/files/styles/two_column_large/public/simons%20foundation.png?itok=l439OKcp'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "FMQ6B9GY6Ney"
   },
   "source": [
    "---\n",
    "# Tutorial Objectives\n",
    "\n",
    "*Estimated timing of tutorial: 80 minutes*\n",
    "\n",
    "In this tutorial, we will cover aspects of calculus that will be frequently used in the main NMA course. We assume that you have some familiarty with calculus, but may be a bit rusty or may not have done much practice.  Specifically, the objectives of this tutorial are:\n",
    "\n",
    "*   Get an intuitive understanding of derivative and integration operations\n",
    "*   Learn to calculate the derivatives of 1- and 2-dimensional functions/signals numerically\n",
    "*   Familiarize with the concept of neuron transfer function in 1- and 2-dimensions.\n",
    "*   Familiarize with the idea of numerical integration using Riemann sum\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEsu36Yb6Ne2"
   },
   "source": [
    "\n",
    "  If you want a vidio explanation of (Why do we care about calculus?) from Neuromarch academy , check this link : (https://youtube.com/watch?v=NZwfH_dG2wI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "1e44XYsY6NfC"
   },
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y7ZTSyvGF0zi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "id": "3ZmPeKd_6NfF"
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "!pip install sympy --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "id": "nJeuEGit6NfL"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import scipy.optimize as opt  # import root-finding algorithm\n",
    "import sympy as sp  # Python toolbox for symbolic maths\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Toolbox for rendring 3D figures\n",
    "from mpl_toolkits import mplot3d  # Toolbox for rendring 3D figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "id": "9utesqoR6NfU"
   },
   "outputs": [],
   "source": [
    "# @title Figure Settings\n",
    "import ipywidgets as widgets  # interactive display\n",
    "from ipywidgets import interact\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "# use NMA plot style\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle\")\n",
    "my_layout = widgets.Layout()\n",
    "\n",
    "fig_w, fig_h = 12, 4.5\n",
    "my_fontsize = 16\n",
    "my_params = {'axes.labelsize': my_fontsize,\n",
    "          'axes.titlesize': my_fontsize,\n",
    "          'figure.figsize': [fig_w, fig_h],\n",
    "          'font.size': my_fontsize,\n",
    "          'legend.fontsize': my_fontsize-4,\n",
    "          'lines.markersize': 8.,\n",
    "          'lines.linewidth': 2.,\n",
    "          'xtick.labelsize': my_fontsize-2,\n",
    "          'ytick.labelsize': my_fontsize-2}\n",
    "\n",
    "plt.rcParams.update(my_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "id": "jJAWIdqr6NfY"
   },
   "outputs": [],
   "source": [
    "# @title Plotting Functions\n",
    "def move_sympyplot_to_axes(p, ax):\n",
    "    backend = p.backend(p)\n",
    "    backend.ax = ax\n",
    "    backend.process_series()\n",
    "    backend.ax.spines['right'].set_color('none')\n",
    "    backend.ax.spines['bottom'].set_position('zero')\n",
    "    backend.ax.spines['top'].set_color('none')\n",
    "    plt.close(backend.fig)\n",
    "\n",
    "\n",
    "def plot_functions(function, show_derivative, show_integral):\n",
    "\n",
    "  # For sympy we first define our symbolic variable\n",
    "  x, y, z, t, f = sp.symbols('x y z t f')\n",
    "\n",
    "  # We define our function\n",
    "  if function == 'Linear':\n",
    "    f = -2*t\n",
    "    name = r'$-2t$'\n",
    "  elif function == 'Parabolic':\n",
    "    f =  t**2\n",
    "    name = r'$t^2$'\n",
    "  elif function == 'Exponential':\n",
    "    f =  sp.exp(t)\n",
    "    name = r'$e^t$'\n",
    "  elif function == 'Sine':\n",
    "    f =  sp.sin(t)\n",
    "    name = r'$sin(t)$'\n",
    "  elif function == 'Sigmoid':\n",
    "    f = 1/(1 + sp.exp(-(t-5)))\n",
    "    name = r'$\\frac{1}{1+e^{-(t-5)}}$'\n",
    "\n",
    "  if show_derivative and not show_integral:\n",
    "    # Calculate the derivative of sin(t) as a function of t\n",
    "    diff_f = sp.diff(f)\n",
    "    print('Derivative of', f, 'is ', diff_f)\n",
    "\n",
    "    p1 = sp.plot(f, diff_f, show=False)\n",
    "    p1[0].line_color='r'\n",
    "    p1[1].line_color='b'\n",
    "    p1[0].label='Function'\n",
    "    p1[1].label='Derivative'\n",
    "    p1.legend=True\n",
    "    p1.title = 'Function = ' + name + '\\n'\n",
    "    p1.show()\n",
    "  elif show_integral and not show_derivative:\n",
    "\n",
    "    int_f = sp.integrate(f)\n",
    "    int_f = int_f - int_f.subs(t, -10)\n",
    "    print('Integral of', f, 'is ', int_f)\n",
    "\n",
    "\n",
    "    p1 = sp.plot(f, int_f, show=False)\n",
    "    p1[0].line_color='r'\n",
    "    p1[1].line_color='g'\n",
    "    p1[0].label='Function'\n",
    "    p1[1].label='Integral'\n",
    "    p1.legend=True\n",
    "    p1.title = 'Function = ' + name + '\\n'\n",
    "    p1.show()\n",
    "\n",
    "\n",
    "  elif show_integral and show_derivative:\n",
    "\n",
    "    diff_f = sp.diff(f)\n",
    "    print('Derivative of', f, 'is ', diff_f)\n",
    "\n",
    "    int_f = sp.integrate(f)\n",
    "    int_f = int_f - int_f.subs(t, -10)\n",
    "    print('Integral of', f, 'is ', int_f)\n",
    "\n",
    "    p1 = sp.plot(f, diff_f, int_f, show=False)\n",
    "    p1[0].line_color='r'\n",
    "    p1[1].line_color='b'\n",
    "    p1[2].line_color='g'\n",
    "    p1[0].label='Function'\n",
    "    p1[1].label='Derivative'\n",
    "    p1[2].label='Integral'\n",
    "    p1.legend=True\n",
    "    p1.title = 'Function = ' + name + '\\n'\n",
    "    p1.show()\n",
    "\n",
    "  else:\n",
    "\n",
    "    p1 = sp.plot(f, show=False)\n",
    "    p1[0].line_color='r'\n",
    "    p1[0].label='Function'\n",
    "    p1.legend=True\n",
    "    p1.title = 'Function = ' + name + '\\n'\n",
    "    p1.show()\n",
    "\n",
    "\n",
    "def plot_alpha_func(t, f, df_dt):\n",
    "\n",
    "  plt.figure()\n",
    "  plt.subplot(2,1,1)\n",
    "  plt.plot(t, f, 'r', label='Alpha function')\n",
    "  plt.xlabel('Time (au)')\n",
    "  plt.ylabel('Voltage')\n",
    "  plt.title('Alpha function (f(t))')\n",
    "  #plt.legend()\n",
    "\n",
    "  plt.subplot(2,1,2)\n",
    "  plt.plot(t, df_dt, 'b', label='Derivative')\n",
    "  plt.title('Derivative of alpha function')\n",
    "  plt.xlabel('Time (au)')\n",
    "  plt.ylabel('df/dt')\n",
    "  #plt.legend()\n",
    "\n",
    "\n",
    "def plot_charge_transfer(t, PSP, numerical_integral):\n",
    "\n",
    "  fig, axes = plt.subplots(1, 2)\n",
    "\n",
    "  axes[0].plot(t, PSP)\n",
    "  axes[0].set(xlabel = 't', ylabel = 'PSP')\n",
    "\n",
    "  axes[1].plot(t, numerical_integral)\n",
    "  axes[1].set(xlabel = 't', ylabel = 'Charge Transferred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "Ti8TJf4C6Nfl"
   },
   "source": [
    "---\n",
    "# Section 1: What is differentiation and integration?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rt9HAgxo6Nfm"
   },
   "source": [
    "If you want a video explanation of (A geometrical interpretation of differentiation and integration) from Neuromatch Academy, check this link:(https://youtube.com/watch?v=uQjwr9RQaEs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "-bUM50DN6Nfr"
   },
   "source": [
    "This video covers the definition of differentiation and integration, highlights the geometrical interpretation of each, and introduces the idea of eigenfunctions.\n",
    "\n",
    "<details>\n",
    "<summary> <font color='blue'>Click here for text recap of video </font></summary>\n",
    "\n",
    "Calculus is a part of mathematics concerned with **continous change**. There are two branches of calculus: differential calculus and integral calculus. \n",
    "\n",
    "Differentiation of a function $f(t)$ gives you the derivative of that function $\\frac{d(f(t))}{dt}$. A derivative captures how sensitive a function is to slight changes in the input for different ranges of inputs. Geometrically, the derivative of a function at a certain input is the slope of the function at that input. For example, as you drive, the distance traveled changes continuously with time. The derivative of the distance traveled with respect to time is the velocity of the vehicle at each point in time. The velocity tells you the rate of change of the distance traveled at different points in time. If you have slow velocity (a small derivative), the distance traveled doesn't change much for small changes in time. A high velocity (big derivative) means that the distance traveled changes a lot for small changes in time.\n",
    "\n",
    "The sign of the derivative of a function (or signal) tells whether the signal is increasing or decreasing. For a signal going through changes as a function of time, the derivative will become zero when the signal changes its direction of change (e.g. from increasing to decreasing). That is, at local minimum or maximum values, the slope of the signal will be zero. This property is used in optimizing problems. But we can also use it to find peaks in a signal. \n",
    "\n",
    "Integration can be thought of as the reverse of differentation. If we integrate the velocity with respect to time, we can calculate the distance traveled. By integrating a function, we are basically trying to find functions that would have the original one as their derivative. When we integrate a function, our integral will have an added unknown scalar constant, $C$. \n",
    "For example, if\n",
    "\n",
    "\\begin{equation}\n",
    "g(t) = 1.5t^2 + 4t - 1\n",
    "\\end{equation}\n",
    "\n",
    "our integral function $f(t)$ will be:\n",
    "\n",
    "\\begin{equation}\n",
    "f(t) = \\int g(t) dt = 0.5t^3 + 2t^2 - t + C\n",
    "\\end{equation}\n",
    "\n",
    "This constant exists because the derivative of a constant is 0 so we cannot know what the constant should be. This is an indefinite integral. If we compute a definite integral, that is the integral between two limits of the input, we will not have this unknown constant and the integral of a function will capture the area under the curve of that function between those two limits.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "kSUEY5bh6Nfu"
   },
   "source": [
    "### Interactive Demo 1: Geometrical understanding\n",
    "\n",
    "In the interactive demo below, you can pick different functions to examine in the drop down menu. You can then choose to show the derivative function and/or the integral function. \n",
    "\n",
    "For the integral, we have chosen the unknown constant $C$ such that the integral function at the left x-axis limit is $0$, as $f(t = -10) = 0$. So the integral will reflect the area under the curve starting from that position.\n",
    "\n",
    "For each function:\n",
    "\n",
    "* Examine just the function first. Discuss and predict what the derivative and integral will look like. Remember that derivative = slope of function, integral = area under curve from $t = -10$ to that t.\n",
    "* Check the derivative - does it match your expectations?\n",
    "* Check the integral - does it match your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "id": "8EnNvPzk6Nfz"
   },
   "outputs": [],
   "source": [
    "# @markdown Execute this cell to enable the widget\n",
    "function_options = widgets.Dropdown(\n",
    "    options=['Linear', 'Exponential', 'Sine', 'Sigmoid'],\n",
    "    description='Function',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "derivative = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Show derivative',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "integral = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Show integral',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "def on_value_change(change):\n",
    "    derivative.value = False\n",
    "    integral.value = False\n",
    "\n",
    "function_options.observe(on_value_change, names='value')\n",
    "\n",
    "interact(plot_functions, function = function_options, show_derivative = derivative, show_integral = integral);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "KITfu7PW6Nf5"
   },
   "source": [
    "In the demo above you may have noticed that the derivative and integral of the exponential function is same as the exponential function itself.\n",
    "\n",
    "Some functions like the exponential function, when differentiated or integrated, equal a scalar times the same function. This is a similar idea to eigenvectors of a matrix being those that, when multipled by the matrix, equal a scalar times themselves, as you saw yesterday!\n",
    "\n",
    "When \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{d(f(t))}{dt} = a \\cdot f(t)\\text{,}\n",
    "\\end{equation}\n",
    "\n",
    "we say that $f(t)$ is an **eigenfunction** for derivative operator, where $a$ is a scaling factor. Similarly, when \n",
    "\n",
    "\\begin{equation}\n",
    "\\int f(t)dt = a \\cdot f(t)\\text{,} \n",
    "\\end{equation}\n",
    "\n",
    "we say that $f(t)$ is an **eigenfunction** for integral operator.\n",
    "\n",
    "As you can imagine, working with eigenfunctions can make mathematical analysis easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "VELAbaXD6NgE"
   },
   "source": [
    "---\n",
    "# Section 2: Analytical & Numerical Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_Bl9it06NgK"
   },
   "source": [
    "If You want a video explanation of (Differentiation) from Neuromatch Academy, check this link : (https://youtube.com/watch?v=sHogZISXGuQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "kJmI-Brj6NgO"
   },
   "source": [
    "\n",
    "In this section, we will delve into how we actually find the derivative of a function, both analytically and numerically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "zPbfLo2z6NgR"
   },
   "source": [
    "## Section 2.1: Analytical Differentiation\n",
    "\n",
    "*Estimated timing to here from start of tutorial: 20 min*\n",
    "\n",
    "When we find the derivative analytically, we are finding the exact formula for the derivative function. \n",
    "\n",
    "To do this, instead of having to do some fancy math every time, we can often consult [an online resource](https://en.wikipedia.org/wiki/Differentiation_rules) for a list of common derivatives, in this case our trusty friend Wikipedia.\n",
    "\n",
    "If I told you to find the derivative of $f(t) = t^3$, you could consult that site and find in Section 2.1, that if $f(t) = t^n$, then $\\frac{d(f(t))}{dt} = nt^{n-1}$. So you would be able to tell me that the derivative of $f(t) = t^3$ is $\\frac{d(f(t))}{dt} = 3t^{2}$.\n",
    "\n",
    "This list of common derivatives often contains only very simple functions. Luckily, as we'll see in the next two sections, we can often break the derivative of a complex function down into the derivatives of more simple components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "NixW7S1A6NgU"
   },
   "source": [
    "### Section 2.1.1: Product Rule\n",
    "\n",
    "Sometimes we encounter functions which are the product of two functions that both depend on the variable. \n",
    "How do we take the derivative of such functions? For this we use the [Product Rule](https://en.wikipedia.org/wiki/Product_rule).\n",
    "\n",
    "\\begin{align}\n",
    "f(t) &= u(t)\\cdot v(t) \\\\\n",
    "\\frac{d(f(t))}{dt} &= v\\cdot \\frac{du}{dt} + u\\cdot \\frac{dv}{dt}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "ocVPTP7x6NgZ"
   },
   "source": [
    "#### Coding Exercise 2.1.1: Derivative of the postsynaptic potential alpha function \n",
    "\n",
    "Let's use the product rule to get the derivative of the post-synaptic potential alpha function. As we saw in Video 3, the shape of the postsynaptic potential is given by the so called alpha function:\n",
    "\n",
    "\\begin{equation}\n",
    "f(t) = t \\cdot \\text{exp}\\left( -\\frac{t}{\\tau} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "Here $f(t)$ is a product of $t$ and $\\text{exp} \\left(-\\frac{t}{\\tau} \\right)$. So we can have $u(t) = t$ and $v(t) = \\text{exp} \\left( -\\frac{t}{\\tau} \\right)$ and use the product rule! \n",
    "\n",
    "We have defined $u(t)$ and $v(t)$ in the code below, in terms of the variable $t$ which is an array of time steps from 0 to 10. Define $\\frac{du}{dt}$ and $\\frac{dv}{dt}$, the compute the full derivative of the alpha function using the product rule. You can always consult wikipedia to figure out $\\frac{du}{dt}$ and $\\frac{dv}{dt}$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "id": "RLJQgHvk6Nga"
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "## TODO for students\n",
    "## Complete all ... in code below and remove\n",
    "raise NotImplementedError(\"Calculate the derivatives\")\n",
    "########################################################################\n",
    "\n",
    "# Define time, time constant\n",
    "t = np.arange(0, 10, .1)\n",
    "tau = 0.5\n",
    "\n",
    "# Compute alpha function\n",
    "f = t * np.exp(-t/tau)\n",
    "\n",
    "# Define u(t), v(t)\n",
    "u_t = t\n",
    "v_t = np.exp(-t/tau)\n",
    "\n",
    "# Define du/dt, dv/dt\n",
    "du_dt = ...\n",
    "dv_dt = ...\n",
    "\n",
    "# Define full derivative\n",
    "df_dt = ...\n",
    "\n",
    "# Visualize\n",
    "plot_alpha_func(t, f, df_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "AbF9f4Se6Ngb"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/precourse/tree/main//tutorials/W0D4_Calculus/solutions/W0D4_Tutorial1_Solution_636667ff.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=1687.0 height=607.0 src=https://raw.githubusercontent.com/NeuromatchAcademy/precourse/main/tutorials/W0D4_Calculus/static/W0D4_Tutorial1_Solution_636667ff_0.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "P2V6SDjg6Ngh"
   },
   "source": [
    "### Section 2.1.2: Chain Rule\n",
    "\n",
    "Many times we encounter situations in which the variable $a$ is changing with time ($t$) and affecting another variable $r$. How can we estimate the derivative of $r$ with respect to $a$ i.e. $\\frac{dr}{da} = ?$\n",
    "\n",
    "To calculate $\\frac{dr}{da}$ we use the [Chain Rule](https://en.wikipedia.org/wiki/Chain_rule).\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{dr}{da} = \\frac{dr}{dt}\\cdot\\frac{dt}{da}\n",
    "\\end{equation}\n",
    "\n",
    "That is, we calculate the derivative of both variables with respect to t and divide that derivative of $r$ by that  derivative of $a$. \n",
    "\n",
    "We can also use this formula to simplify taking derivatives of complex functions! We can make an arbitrary function t so that we can compute more simple derivatives and multiply, as we will see in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "mYEyfW5w6Ngk"
   },
   "source": [
    "#### Math Exercise 2.1.2: Chain Rule\n",
    "\n",
    "Let's say that:\n",
    "\n",
    "\\begin{equation}\n",
    "r(a) = e^{a^4 + 1}\n",
    "\\end{equation}\n",
    "\n",
    "What is $\\frac{dr}{da}$? This is a more complex function so we can't simply consult a table of common derivatives. Can you use the chain rule to help?\n",
    "\n",
    "**Hint:** we didn't define t but you could set t equal to the function in the exponent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "AX6m2a3W6Ngo"
   },
   "source": [
    "### Section 2.2.3: Derivatives in Python using SymPy\n",
    "\n",
    "There is a useful Python library for getting the analytical derivatives of functions: SymPy. We actually used this in Interactive Demo 1, under the hood.\n",
    "\n",
    "See the following cell for an example of setting up a sympy function and finding the derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "id": "Wc4r20tQ6Ngo"
   },
   "outputs": [],
   "source": [
    "# For sympy we first define our symbolic variables\n",
    "f, t = sp.symbols('f, t')\n",
    "\n",
    "# Function definition (sigmoid)\n",
    "f = 1/(1 + sp.exp(-(t-5)))\n",
    "\n",
    "# Get the derivative\n",
    "diff_f = sp.diff(f)\n",
    "\n",
    "# Print the resulting function\n",
    "print('Derivative of', f, 'is ', diff_f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "nq-hVvP1MZqa",
    "KCWR4wciMZqb",
    "tcLiKp-AMZqc",
    "8-bz4N8eMZqd",
    "9-BbBcZPMZqf",
    "L0_P-VNEMZqf",
    "Yx7vP4zvMZqg",
    "RACJEWwMMZqg",
    "dXFmgh9UMZqh",
    "pMQS0U54MZqi",
    "ZLb9XPF_VvPm",
    "c0oilOjOV0KZ"
   ],
   "name": "W2D3_Tutorial",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
